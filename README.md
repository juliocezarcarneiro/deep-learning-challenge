# ü§ñ Alphabet Soup: Neural Network vs. XGBoost for Donation Success Prediction

## üìå Project Overview

The goal of this project is to build a deep learning model to predict whether charitable donation applications submitted to Alphabet Soup will be successful. Using historical application data, we frame this as a **binary classification** problem (`IS_SUCCESSFUL = 1` or `0`) and benchmark our model against a **75% accuracy target**.

Additionally, we compare the neural network's performance to a tuned **XGBoost model**, to determine which approach is more effective for this problem.

---

## üß† Neural Network Model Report

### üîÑ Data Preprocessing

- **Target Variable**:  
  - `IS_SUCCESSFUL`

- **Feature Variables**:  
  - All other columns in the cleaned dataset after encoding categorical features with `pd.get_dummies`.

- **Removed Variables**:
  - `EIN`, `NAME` ‚Äî dropped as identifiers with no predictive value.
  - Rare values in `CLASSIFICATION` and `APPLICATION_TYPE` were grouped under `"Other"` to reduce dimensionality and noise.

---

### üõ†Ô∏è Model Architecture & Training

- **Input Layer**:
  - Based on the number of features after scaling.

- **Hidden Layers**:
  - Layer 1: 128 neurons, ReLU
  - Layer 2: 64 neurons, ReLU

- **Output Layer**:
  - 1 neuron, Sigmoid activation (for binary classification)

- **Compilation**:
  - Loss Function: `binary_crossentropy`
  - Optimizer: `adam`
  - Metric: `accuracy`

- **Preprocessing**:
  - Numeric features scaled with `StandardScaler`
  - Categorical features encoded with `pd.get_dummies`

---

### üìâ Neural Network Performance

- **Accuracy**: `73.05%`
- **Loss**: `0.5713`

#### ‚öôÔ∏è Optimization Steps

- Experimented with different:
  - Hidden layer sizes and neuron counts
  - Epochs and batch sizes
  - Feature groupings for rare classes
- Scaled inputs for better training stability

‚õî **Conclusion**: Despite tuning, the neural network fell short of the 75% accuracy benchmark.

---

## üîÅ XGBoost Benchmark (with Optuna Tuning)

- **Optimization**:
  - Used `Optuna` with 50 trials to fine-tune:
    - `n_estimators`, `learning_rate`, `max_depth`, `subsample`, `colsample_bytree`, `reg_alpha`, `reg_lambda`

- **Final Evaluation**:
  - Accuracy: `75.02%`
  - ROC AUC: `0.8155`

- **Classification Report**:
  - Precision, recall, and F1-scores exceeded those of the neural network.

---

## üßæ Summary & Recommendation

The neural network performed reasonably well, achieving **73.05% accuracy**, but did not meet the benchmark. The **XGBoost model**, optimized with Optuna, reached **75.02% accuracy** and demonstrated better performance overall.

### ‚úÖ Recommendation:
For this binary classification task:
- Use **XGBoost** due to:
  - Higher accuracy
  - Better handling of categorical and imbalanced features
  - Easier interpretability and feature importance extraction

---

## üíª Technologies Used

- Python
- TensorFlow / Keras
- Scikit-learn
- XGBoost
- Optuna (for hyperparameter tuning)
- Pandas, NumPy, Matplotlib

---

## üíª Technical References

* TensorFlow Documentation (2023). Keras API reference.
https://www.tensorflow.org/api_docs/python/tf/keras

* Scikit-learn Developers (2023). Scikit-learn documentation.
https://scikit-learn.org/stable/documentation.html

* Pandas Development Team (2023). Pandas documentation.
https://pandas.pydata.org/docs/

* Google Colab (2023). Official documentation.
https://colab.research.google.com/

* Google AI (2023). Gemini API documentation.
https://ai.google.dev/

* NumPy Documentation (2023). NumPy user guide.
https://numpy.org/doc/

* Python Documentation (2023). Official Python docs.
https://docs.python.org/3/

* GitHub Documentation (2023). Version control platform.
https://docs.github.com/

* Jupyter Documentation (2023). Notebook environment docs.
https://docs.jupyter.org/



